{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artctoc data\n",
    "\n",
    "Cleaning GPS and analysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Faire la jointure entre les données individus et les positions GPS\n",
    "\n",
    "le pb est que les données attributaires des oiseaux ont une GLS ID qui ne correspond pas directement aux identifiants du fichier GPS (id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year', 'Bird_ID', 'GLS_ID', 'Sex', 'Date', 'Nest', 'Nest_content',\n",
       "       'Capture_method', 'Headbill', 'Culmen', 'Wing', 'Right_tarsus', 'Mass',\n",
       "       'Index_body_condition', 'Muscle_Pectoral', 'Score_Personal', 'Long_Egg',\n",
       "       'Long_Egg_cm', 'Larg_Egg', 'Larg_Egg_cm', 'Vol_egg', 'Arrival_date',\n",
       "       'Arrival_date_num', 'Arrival_date_propre', 'Arrival_date_propre_num',\n",
       "       'date_enter_nest', 'date_enter_nest_num', 'Hatch_date',\n",
       "       'Hatch_date_num', 'weigh_hatching', 'Hatching_success',\n",
       "       'Chick_mass_gain_(g/d)_1st_15d', 'pente_chick_growth_1-15d',\n",
       "       'N_SIA_Blood', 'N_SIA_head_Feather', 'C_SIA_Blood',\n",
       "       'C_SIA_head_Feather', 'Chick_sex', 'Cortico', 'Hg_HF', 'Hg_blood',\n",
       "       'Season_Hg_Blood', 'Hg_BF', 'BF_side', 'Long_Median_15Oct_20Fev',\n",
       "       'Lat_Median_15Oct_20Fev', 'Long_Median_DecJan', 'Lat_Median_DecJan',\n",
       "       'Long_Median_Dec_20Fev', 'Lat_Median_Dec_20Fev', 'd_PL_15Oct_20Fev',\n",
       "       'd_PL_DecJan', 'd_PL_1Dec_20Fev', 'Max_d_col__Dec_Jan',\n",
       "       'Max_d_col__15Oct_20Fev', 'PL_Lat', 'PL_Long'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_excel(\"C:\\Travail\\Enseignement\\Cours_M2_python\\Projet_Arctox\\data for analyses_2010_2011_analyses.xls\", \n",
    "#                            sheet_name=\"data for analyses_2010_2011_ana\")\n",
    "#https://github.com/cplumejeaud/M2_python/raw/refs/heads/main/data/arctox/data%20for%20analyses_2010_2011_analyses.xls\n",
    "df = pd.read_excel(\"https://github.com/cplumejeaud/M2_python/raw/refs/heads/main/data/arctox/data%20for%20analyses_2010_2011_analyses.xls\", \n",
    "                            sheet_name=\"data for analyses_2010_2011_ana\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3506, 3597, 3600, 3603, 3604, 3607, 3611, 3613, 3614, 3617, 3620,\n",
       "       3621, 3626, 3632, 3635, 3638, 3639, 3641, 3642, 3644, 3646, 3647,\n",
       "       3648, 3649, 3651, 3653, 3654, 3658, 3660, 3665, 3666, 3669, 3671,\n",
       "       3673, 3674, 3675, 3680, 3682, 3683, 3687, 3688, 3689, 3698, 3699,\n",
       "       3701, 3702, 'Mk18-17585', 'MK12-12A155', 'MK12-12A159',\n",
       "       'MK18-17589', 'SO-26', 'SO-19', 'SO-2', 'SO-11', 'SO-5', 'SO-1',\n",
       "       'SO-32', 'MK12-12A148', 'MK12-12A154', 'MK12-12A153', 'MK18-17584',\n",
       "       'Mk12-12A162', 'Mk12-12A149', 'Mk12-12A163', 'SO-29',\n",
       "       'MK12-12A157', 'MK18-17586', 'SO-13', 'SO-15', 'Mk14-3656',\n",
       "       'Mk14-3679', 'MK18-17587', 'MK12-12A150', 'MK18-17582',\n",
       "       'MK18-17577', 'MK12-12A158', 'MK12-12A151', 'Mk14-3668', 'SO-36',\n",
       "       'SO-7', nan], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.GLS_ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  148   154   151   149   157   162   159  3668  3656 17577  3679 17584\n",
      " 17582 17585 17589 17587 17586   158]\n",
      "(4641, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sqlalchemy import create_engine, text as sql_text\n",
    "from sqlalchemy.sql import quoted_name, literal_column \n",
    "\n",
    "\n",
    "connection = create_engine('postgresql://christine:christineM2@localhost:8009/savoie', connect_args={'options': '-csearch_path={}'.format('arctic_christine')})\n",
    "ORM_conn = connection.connect()\n",
    "\n",
    "query = sql_text(\"\"\"select id, pointgps, timestampgps from kap_hoegh_gls  \"\"\")\n",
    "\n",
    "gps = pd.read_sql_query(con= ORM_conn, sql=query) \n",
    " \n",
    "gps.head()\n",
    "\n",
    "\n",
    "print(gps['id'].unique())\n",
    "print(gps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Lire le fichier complet des positions GPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28833, 13)\n",
      "Index(['id', 'sex', 'date', 'time', 'lat', 'long', 'week', 'clean_lat',\n",
      "       'clean_long', 'smooth_lat', 'smooth_long', 'distance_to_colony',\n",
      "       'shoreline_distance', 'geometry'],\n",
      "      dtype='object')\n",
      "(28833, 14)\n",
      "[  148   149   151   154   157   158   159   162 17577 17582 17584 17585\n",
      " 17586 17587 17589  3597  3600  3603  3604  3606  3607  3611  3613  3614\n",
      "  3617  3620  3621  3626  3632  3635  3638  3639  3641  3642  3644  3646\n",
      "  3647  3648  3649  3651  3653  3654  3656  3658  3660  3665  3666  3668\n",
      "  3669  3671  3674  3675  3679  3680  3682  3683  3687  3688  3689  3690\n",
      "  3698  3701]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_gps = pd.read_csv(\"C:\\Travail\\Enseignement\\Cours_M2_python\\\\2023\\data\\kap_hoegh_gls_complet.csv\", sep=\";\", encoding='utf-8')\n",
    "print(df_gps.shape)\n",
    "gps = gpd.GeoDataFrame(\n",
    "    df_gps, geometry=gpd.points_from_xy(df_gps.long, df_gps.lat), crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "\n",
    "print(gps.columns)\n",
    "print(gps.shape)\n",
    "print(gps.id.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer les positions GPS des oiseaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le nettoyage, nous allons nous inspirer d'une approche par lissage temporel, pour supprimer les latitudes anormales\n",
    "- voir le package https://pypi.org/project/tsmoothie/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://stackoverflow.com/questions/20618804/how-to-smooth-a-curve-in-the-right-way\n",
    "\n",
    "x = np.linspace(0,2*np.pi,100) #timestampgps\n",
    "y = np.sin(x) + np.random.random(100) * 0.2 #lat\n",
    "\n",
    "# operate smoothing\n",
    "import tsmoothie \n",
    "from tsmoothie.smoother import ConvolutionSmoother\n",
    "\n",
    "smoother = ConvolutionSmoother(window_len=5, window_type='ones')\n",
    "smoother.smooth(y)\n",
    "\n",
    "# generate intervals\n",
    "low, up = smoother.get_intervals('sigma_interval', n_sigma=2)\n",
    "\n",
    "### End of adaptation\n",
    "\n",
    "# plot the smoothed timeseries with intervals\n",
    "# Avec BOKEH \n",
    "from bokeh.plotting import show, figure, output_file, output_notebook\n",
    "\n",
    "#output_notebook() \n",
    "output_file(\"smoothed_data_example.html\")\n",
    "\n",
    "p = figure(width=800, height=400)\n",
    "\n",
    "# add a line renderer for smoothed line\n",
    "p.line(x, smoother.smooth_data[0], line_width =3, color='red')\n",
    "p.circle(x, smoother.data[0], size =3, fill_color=\"white\")\n",
    "# add an area between low and up smoothed data\n",
    "p.varea(x=x,y1=low[0], y2=up[0], alpha=0.3)\n",
    "\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. vous avez besoin d'un axe temporel : une colonne timestamp\n",
    "\n",
    "- https://realpython.com/python-datetime/ \n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html\n",
    "- https://www.delftstack.com/fr/howto/python-pandas/how-to-convert-dataframe-column-to-datetime-in-pandas/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Timestamp.combine(gps.date, gps.time)\n",
    "#print(gps.date+' '+gps.time)\n",
    "gps['timestamp'] = gps.date+' '+gps.time\n",
    "format_string = \"%Y-%m-%d %H:%M:%S\"\n",
    "from datetime import datetime\n",
    "\n",
    "gps.timestamp = gps.timestamp.apply(lambda x: datetime.strptime(x, format_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mettre en oeuvre un lissage sur les latitudes\n",
    "Adapt the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From now, adapt the code for smoothing bad latitudes\n",
    "## https://stackoverflow.com/questions/20618804/how-to-smooth-a-curve-in-the-right-way\n",
    "\n",
    "x = gps.timestamp.values #timestampgps\n",
    "y = gps.clean_lat.values #df.loc[:,['clean_lat']].values\n",
    "\n",
    "# operate smoothing\n",
    "import tsmoothie \n",
    "from tsmoothie.smoother import ConvolutionSmoother\n",
    "\n",
    "#First simplest one : moving average of span = 5\n",
    "#smoother = ConvolutionSmoother(window_len=5, window_type='ones')\n",
    "#smoother.smooth(y)\n",
    "\n",
    "#https://pypi.org/project/tsmoothie/\n",
    "#https://fr.wikipedia.org/wiki/Fen%C3%AAtrage \n",
    "#Second one : moving weighted average of span = 10, using hamming function\n",
    "smoother = ConvolutionSmoother(window_len=20, window_type='hamming')\n",
    "smoother.smooth(y)\n",
    "\n",
    "# generate intervals\n",
    "low, up = smoother.get_intervals('sigma_interval', n_sigma=2)\n",
    "\n",
    "### End of adaptation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the smoothed timeseries with intervals\n",
    "from bokeh.plotting import show, figure, output_file, output_notebook\n",
    "\n",
    "#output_notebook() \n",
    "output_file(\"smoothed_data.html\")\n",
    "\n",
    "p = figure(width=1600, height=800, x_axis_type='datetime')\n",
    "\n",
    "# add a line renderer for smoothed line\n",
    "p.line(x, smoother.smooth_data[0], line_width =3, color='blue')\n",
    "p.circle(x, smoother.data[0], size =3, fill_color=\"white\")\n",
    "# add an area between low and up smoothed data\n",
    "p.varea(x=x,y1=low[0], y2=up[0], alpha=0.3)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the result\n",
    "gps['smooth_lat'] = smoother.smooth_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joindre GPS et données d'analyse\n",
    "\n",
    "- A gauche, les positions GPS\n",
    "- A droite, les données attributaires\n",
    "\n",
    "Pb : il faut un peu nettoyer df.GLS_ID.unique() en supprimant\n",
    "- MK12-12A\n",
    "- MK18-\n",
    "- MK14-\n",
    "- SO-\n",
    "du fichier d'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'sex', 'date', 'time', 'lat', 'long', 'week', 'clean_lat',\n",
      "       'clean_long', 'smooth_lat', 'smooth_long', 'distance_to_colony',\n",
      "       'shoreline_distance', 'geometry', 'timestamp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(gps.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "['3506' '3597' '3600' '3603' '3604' '3607' '3611' '3613' '3614' '3617'\n",
      " '3620' '3621' '3626' '3632' '3635' '3638' '3639' '3641' '3642' '3644'\n",
      " '3646' '3647' '3648' '3649' '3651' '3653' '3654' '3658' '3660' '3665'\n",
      " '3666' '3669' '3671' '3673' '3674' '3675' '3680' '3682' '3683' '3687'\n",
      " '3688' '3689' '3698' '3699' '3701' '3702' '17585' '155' '159' '17589'\n",
      " '26' '19' '2' '11' '5' '1' '32' '148' '154' '153' '17584' '162' '149'\n",
      " '163' '29' '157' '17586' '13' '15' '3656' '3679' '17587' '150' '17582'\n",
      " '17577' '158' '151' '3668' '36' '7' 'NAN']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(type(df.GLS_ID.values))\n",
    "# Caster en str ce mélange de numerique et de chaînes de caractères\n",
    "df.GLS_ID = df.GLS_ID.apply(lambda _: str(_))\n",
    "print(type(df.GLS_ID.values))\n",
    "\n",
    "clean_GLSID = []\n",
    "for i, r in df.iterrows():\n",
    "    clean_gls_id = r.GLS_ID.upper().replace('MK12-12A', '').replace('MK14-', '').replace('MK18-', '').replace('SO-', '')\n",
    "    clean_GLSID = np.append(clean_GLSID, clean_gls_id)\n",
    "\n",
    "print(pd.unique(clean_GLSID))\n",
    "df['clean_GLSID'] = clean_GLSID\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On fait la jointure sur des type identiques (id passe de int à string)\n",
    "gps.id = gps.id.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 58)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['3506', '3597', '3600', '3603', '3604', '3607', '3611', '3613',\n",
       "       '3614', '3617', '3620', '3621', '3626', '3632', '3635', '3638',\n",
       "       '3639', '3641', '3642', '3644', '3646', '3647', '3648', '3649',\n",
       "       '3651', '3653', '3654', '3658', '3660', '3665', '3666', '3669',\n",
       "       '3671', '3673', '3674', '3675', '3680', '3682', '3683', '3687',\n",
       "       '3688', '3689', '3698', '3699', '3701', '3702', '17585', '155',\n",
       "       '159', '17589', '26', '19', '2', '11', '5', '1', '32', '148',\n",
       "       '154', '153', '17584', '162', '149', '163', '29', '157', '17586',\n",
       "       '13', '15', '3656', '3679', '17587', '150', '17582', '17577',\n",
       "       '158', '151', '3668', '36', '7'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Supprimer les valeurs NAN du fichier d'Analyses\n",
    "\n",
    "#df.dropna(subset=['clean_GLSID'], inplace = True)\n",
    "df  = df[df.clean_GLSID != 'NAN']\n",
    "print(df.shape)\n",
    "df.clean_GLSID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 58)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['3506', '3597', '3600', '3603', '3604', '3607', '3611', '3613',\n",
       "       '3614', '3617', '3620', '3621', '3626', '3632', '3635', '3638',\n",
       "       '3639', '3641', '3642', '3644', '3646', '3647', '3648', '3649',\n",
       "       '3651', '3653', '3654', '3658', '3660', '3665', '3666', '3669',\n",
       "       '3671', '3673', '3674', '3675', '3680', '3682', '3683', '3687',\n",
       "       '3688', '3689', '3698', '3699', '3701', '3702', '17585', '155',\n",
       "       '159', '17589', '26', '19', '2', '11', '5', '1', '32', '148',\n",
       "       '154', '153', '17584', '162', '149', '163', '29', '157', '17586',\n",
       "       '13', '15', '3656', '3679', '17587', '150', '17582', '17577',\n",
       "       '158', '151', '3668', '36', '7'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.clean_GLSID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jointure\n",
    "\n",
    "data = gps.join(df.set_index('clean_GLSID'), on='id', lsuffix='_gps', rsuffix='_ana')\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "print(data.columns)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poursuivre l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Index_body_condition.dropna()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
